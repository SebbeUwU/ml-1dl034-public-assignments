{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing \u001b[95mwith \u001b[92mpretty \u001b[91mcolors\u001b[0m\u001b[1m!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Helper to print with prettier colors\n",
    "class c:\n",
    "    PURPLE = '\\033[95m'\n",
    "    BLUE = '\\033[94m'\n",
    "    CYAN = '\\033[96m'\n",
    "    GREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    END = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    \n",
    "print(f\"Printing {c.PURPLE}with {c.GREEN}pretty {c.FAIL}colors{c.END}{c.BOLD}!{c.END}\")\n",
    "\n",
    "# Loading the data\n",
    "df = pd.read_csv('./train_dataset.csv')\n",
    "#df = sns.load_dataset(\"./train_dataset.csv/train_dataset\")\n",
    "columns_old = df.columns\n",
    "columns_new = []\n",
    "\n",
    "for i, col in enumerate(columns_old):\n",
    "    #col = re.sub('[(].+[)]', '', col)\n",
    "    #col = col.lower()\n",
    "    col = col.strip()\n",
    "    #col = re.sub('\\s', '_', col)\n",
    "    columns_new.append(col)\n",
    "\n",
    "df.columns = columns_new\n",
    "#small_df = df.sample(frac=0.1)\n",
    "#df = df.drop(columns=df.columns[14:54], axis=1)\n",
    "df_soil_types = df.iloc[:, 14:54]\n",
    "df_wilderness = df.iloc[:, 10:14]\n",
    "df_hillshade = df.iloc[:, 6:9]\n",
    "df_azimuth = df[[\"Aspect (azimuth)\"]]\n",
    "\n",
    "df_vals = df.iloc[:, 0:10]\n",
    "\n",
    "df_to_normalize = df.iloc[:, [0, 2, 3, 4, 5, 9]]\n",
    "all_normal = pd.DataFrame(preprocessing.normalize(df_vals))\n",
    "\n",
    "\n",
    "\n",
    "df_all_normal = pd.concat([all_normal, df[\"Forest Cover Type Classes\"]], axis=1)\n",
    "df_no_normal = pd.concat([df_vals, df[\"Forest Cover Type Classes\"]], axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = preprocessing.normalize(df_to_normalize)\n",
    "hillshade = preprocessing.MinMaxScaler().fit_transform(df_hillshade)\n",
    "azimuth = preprocessing.MinMaxScaler().fit_transform(df_azimuth)\n",
    "df_normalized = pd.DataFrame({\n",
    "                                \"Elevation (meters)\": normalized[:, 0],\n",
    "                                \"Slope (degrees)\": normalized[:, 1],\n",
    "                                \"Horizontal_Distance_To_Hydrology (meters)\": normalized[:, 2],\n",
    "                                \"Vertical_Distance_To_Hydrology (meters)\": normalized[:, 3],\n",
    "                                \"Horizontal_Distance_To_Roadways(meters)\": normalized[:, 4],\n",
    "                                \"Horizontal_Distance_To_Fire_Points (meters)\": normalized[:, 5],\n",
    "                            })\n",
    "\n",
    "df_hillshade = pd.DataFrame({\n",
    "                                \"Hillshade_9am (0-255)\": hillshade[:, 0],\n",
    "                                \"Hillshade_Noon (0-255)\": hillshade[:, 1],\n",
    "                                \"Hillshade_3pm (0-255)\": hillshade[:, 2]\n",
    "                            })\n",
    "\n",
    "df_azimuth = pd.DataFrame({\"Aspect (azimuth)\": azimuth[:,0]})\n",
    "\n",
    "soil_type_numerical = np.argmax(df_soil_types, axis=1)\n",
    "df_soil_type_numerical = pd.DataFrame({\"Soil Type\": soil_type_numerical})\n",
    "\n",
    "wilderness_numerical = np.argmax(df_wilderness, axis=1)\n",
    "df_wilderness_numerical = pd.DataFrame({\"Wilderness\": wilderness_numerical})\n",
    "\n",
    "df_preprocessed_numerical = pd.concat([df_normalized, df_azimuth, df_hillshade, df_wilderness_numerical, df_soil_type_numerical, df[\"Forest Cover Type Classes\"]], axis=1)\n",
    "df_preprocessed = pd.concat([df_normalized, df_azimuth, df_hillshade, df_wilderness, df_soil_types, df[\"Forest Cover Type Classes\"]], axis=1)\n",
    "df_numerical = pd.concat([df_to_normalize, df_azimuth, df_hillshade, df_wilderness_numerical, df_soil_type_numerical, df[\"Forest Cover Type Classes\"]], axis=1)\n",
    "df[\"Forest Cover Type Classes\"].value_counts()\n",
    "\n",
    "df_only_soil = pd.concat([df_to_normalize, df[\"Forest Cover Type Classes\"]], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for i in range(1,8):\n",
    "    new_df = df[df[\"Forest Cover Type Classes\"] == i].sample(n=2000)\n",
    "    df_list.append(new_df)\n",
    "df_sampled = pd.concat(df_list)\n",
    "\n",
    "df_list = []\n",
    "for i in range(1,8):\n",
    "    new_df = df_numerical[df_numerical[\"Forest Cover Type Classes\"] == i].sample(n=2000)\n",
    "    df_list.append(new_df)\n",
    "df_sampled_numerical = pd.concat(df_list)\n",
    "\n",
    "\n",
    "df_list = []\n",
    "for i in range(1,8):\n",
    "    new_df = df_preprocessed_numerical[df_preprocessed_numerical[\"Forest Cover Type Classes\"] == i].sample(n=2000)\n",
    "    df_list.append(new_df)\n",
    "df_sampled_preprocessed_numerical = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "def under_sample(df):\n",
    "\n",
    "    X = df.drop(df.columns[-1], axis=1)\n",
    "    y = df[df.columns[-1]]\n",
    "\n",
    "    rus = RandomUnderSampler(sampling_strategy=\"not minority\")\n",
    "\n",
    "    X_res, y_res = rus.fit_resample(X, y)\n",
    "\n",
    "    return X_res, y_res\n",
    "\n",
    "\n",
    "def over_sample(df):\n",
    "\n",
    "    X = df.drop(df.columns[-1], axis=1)\n",
    "    y = df[df.columns[-1]]\n",
    "\n",
    "    ros = RandomOverSampler(sampling_strategy=\"not majority\")\n",
    "\n",
    "    X_res, y_res = ros.fit_resample(X, y)\n",
    "\n",
    "    return X_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest\n",
      "0.827\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 28\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#do(svm.LinearSVC(random_state=42, dual=\"auto\"), \"SVC-linear\")\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#do(svm.SVC(random_state=42), \"SVC\")\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensemble\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRandomForestClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRandomForest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#do(tree.DecisionTreeClassifier(random_state=42), \"DecisionTree\")\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#do(neighbors.KNeighborsClassifier(), \"kNN\")\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#SVC-linear\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# in 3m 54.9s, 14000 samples in 3 datasets with 10 folds and random_state=42\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m#22.4\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[43], line 23\u001b[0m, in \u001b[0;36mdo\u001b[1;34m(model, mod_str)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m#clf = ensemble.RandomForestClassifier(random_state=42)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m#model.fit(X_train, y_train)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m#y_pred = model.predict(X_test)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m#acc = accuracy_score(y_test, y_pred)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39mscore\u001b[38;5;241m.\u001b[39mmean())\n\u001b[1;32m---> 23\u001b[0m     feature_scores \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_importances_\u001b[49m, index \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;241m.\u001b[39msort_values(ascending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:673\u001b[0m, in \u001b[0;36mBaseForest.feature_importances_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_importances_\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    654\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;124;03m    The impurity-based feature importances.\u001b[39;00m\n\u001b[0;32m    656\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;124;03m        array of zeros.\u001b[39;00m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    675\u001b[0m     all_importances \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\n\u001b[0;32m    676\u001b[0m         delayed(\u001b[38;5;28mgetattr\u001b[39m)(tree, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_importances_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    677\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\n\u001b[0;32m    678\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mnode_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_importances:\n",
      "File \u001b[1;32mc:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1544\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1541\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm, tree, neighbors, ensemble\n",
    "from sklearn import utils\n",
    "\n",
    "#kNN = neighbors.KNeighborsClassifier()\n",
    "#kNN_score = cross_val_score(kNN, X, y, cv=10)\n",
    "#  df_preprocessed, df_preprocessed_numerical, \n",
    "\n",
    "def do(model, mod_str):\n",
    "    print(mod_str)\n",
    "    for test_df in [df_all_normal, df_no_normal]:\n",
    "        X, y = under_sample(test_df)\n",
    "        #y = test_df[\"Forest Cover Type Classes\"]\n",
    "        #X = test_df.loc[:, test_df.columns != \"Forest Cover Type Classes\"]\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.2, random_state=42)\n",
    "        score = cross_val_score(model, X, y, cv=10)\n",
    "        #clf = ensemble.RandomForestClassifier(random_state=42)\n",
    "        #model.fit(X_train, y_train)\n",
    "        #y_pred = model.predict(X_test)\n",
    "        #acc = accuracy_score(y_test, y_pred)\n",
    "        print(\"%.3f\" %score.mean())\n",
    "        #feature_scores = pd.Series(model.feature_importances_, index = X.columns).sort_values(ascending = False)\n",
    "    print()\n",
    "\n",
    "#do(svm.LinearSVC(random_state=42, dual=\"auto\"), \"SVC-linear\")\n",
    "#do(svm.SVC(random_state=42), \"SVC\")\n",
    "do(ensemble.RandomForestClassifier(random_state=42), \"RandomForest\")\n",
    "#do(tree.DecisionTreeClassifier(random_state=42), \"DecisionTree\")\n",
    "#do(neighbors.KNeighborsClassifier(), \"kNN\")\n",
    "#SVC-linear\n",
    "#df_sampled:                        0.658\n",
    "#df_sampled_numerical:              0.567\n",
    "#df_sampled_preprocessed_numerical: 0.555\n",
    "#\n",
    "#SVC\n",
    "#df_sampled:                        0.641\n",
    "#df_sampled_numerical:              0.623\n",
    "#df_sampled_preprocessed_numerical: 0.528\n",
    "#\n",
    "#RandomForest\n",
    "#df_sampled:                        0.866\n",
    "#df_sampled_numerical:              0.863\n",
    "#df_sampled_preprocessed_numerical: 0.838\n",
    "#\n",
    "#DecisionTree\n",
    "#df_sampled:                        0.787\n",
    "#df_sampled_numerical:              0.790\n",
    "#df_sampled_preprocessed_numerical: 0.762\n",
    "#\n",
    "#kNN\n",
    "#df_sampled:                        0.809\n",
    "#df_sampled_numerical:              0.779\n",
    "#df_sampled_preprocessed_numerical: 0.791\n",
    "\n",
    "# in 3m 54.9s, 14000 samples in 3 datasets with 10 folds and random_state=42\n",
    "#22.4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
